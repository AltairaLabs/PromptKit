apiVersion: promptkit.altairalabs.ai/v1alpha1
kind: Arena
metadata:
  name: vllm-local
  annotations:
    description: "Example demonstrating local high-performance inference with vLLM"
spec:
  prompt_configs:
    - id: assistant
      file: prompts/assistant.yaml
    - id: code-helper
      file: prompts/code-helper.yaml

  providers:
    - file: providers/vllm-local.provider.yaml
    # Uncomment for guided decoding testing
    # - file: providers/vllm-guided-decoding.provider.yaml

  scenarios:
    # Basic functionality tests
    - file: scenarios/basic-chat.scenario.yaml
    - file: scenarios/code-generation.scenario.yaml
    # Uncomment for guided decoding tests
    # - file: scenarios/guided-output.scenario.yaml

  defaults:
    temperature: 0.7
    max_tokens: 2048
    concurrency: 1
    output:
      dir: out
      formats: ["json", "html"]
      html:
        file: report.html
