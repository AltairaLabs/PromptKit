apiVersion: promptkit.altairalabs.ai/v1alpha1
kind: Provider
metadata:
  name: ollama-vision-local
  annotations:
    description: "LLaVA vision model running via local Ollama for multimodal testing"
spec:
  id: "ollama-vision-local"
  type: ollama
  # LLaVA 7B is a vision-language model that can process images
  # Alternative options: llama3.2-vision:11b, bakllava:7b
  model: llava:7b
  # Local Ollama instance
  base_url: "http://localhost:11434"
  additional_config:
    # Keep the model loaded longer since vision models take more time to load
    keep_alive: "10m"
  defaults:
    temperature: 0.7
    max_tokens: 2048
    top_p: 0.9
