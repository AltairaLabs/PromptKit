apiVersion: promptkit.altairalabs.ai/v1alpha1
kind: Provider
metadata:
  name: ollama-llama
  annotations:
    description: "Local Llama 3.2 1B model running via Ollama"
spec:
  id: "ollama-llama"
  type: ollama
  model: llama3.2:1b
  base_url: "http://localhost:11434"
  additional_config:
    # Keep the model loaded in memory for 5 minutes between requests
    keep_alive: "5m"
  defaults:
    temperature: 0.7
    max_tokens: 1024
    top_p: 0.9
