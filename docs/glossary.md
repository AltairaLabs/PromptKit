---
layout: docs
title: Glossary
nav_order: 9
---

# Glossary

Definitions of terms used throughout PromptKit documentation.

## A

**API Key**: Authentication credential for accessing LLM provider services (OpenAI, Anthropic, Google).

**Assertions**: Test conditions in PromptArena that verify LLM responses meet expected criteria.

**Assistant**: Role in message format representing AI/LLM responses.

## C

**Claude**: Anthropic's LLM family (Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus).

**Complete**: Provider method that sends messages to an LLM and returns a response.

**Context**: Information available to the LLM, including system prompts, conversation history, and current message.

**Context Window**: Maximum number of tokens an LLM can process in a single request (e.g., 128K for GPT-4o).

**Conversation**: Series of message exchanges between a user and an LLM.

**Cost Tracking**: Monitoring token usage and calculating expenses for LLM API calls.

## E

**Embedding**: Vector representation of text used for semantic search and similarity.

**Executor**: Component that implements tool execution logic in PromptKit.

**ExecutionContext**: Data structure passed through pipeline middleware containing messages, config, and metadata.

## F

**Fallback**: Strategy where alternative providers are tried if primary provider fails.

**Few-Shot**: Prompt engineering technique providing examples to guide LLM behavior.

**Function Calling**: LLM capability to invoke external functions/tools (also called "tool use").

## G

**Gemini**: Google's LLM family (Gemini 1.5 Pro, Gemini 1.5 Flash).

**GPT**: OpenAI's LLM family (GPT-4o, GPT-4o-mini, GPT-4-turbo).

**Guardrails**: Validation rules that ensure safe and appropriate LLM behavior.

## H

**Hallucination**: When an LLM generates incorrect or fabricated information.

**Human-in-the-Loop (HITL)**: Pattern where human approval is required before executing certain actions.

## L

**LLM (Large Language Model)**: AI model trained on text data to generate human-like responses.

**Load Balancing**: Distributing requests across multiple providers or instances.

## M

**Max Tokens**: Configuration parameter limiting the length of LLM responses.

**MCP (Model Context Protocol)**: Standard protocol for connecting LLMs to external tools and data sources.

**Message**: Unit of conversation with a role (system/user/assistant/tool) and content.

**Middleware**: Component in the pipeline that processes requests before/after LLM execution.

**Mock Provider**: Test provider that returns predefined responses without calling real LLM APIs.

**Multi-turn**: Conversation spanning multiple back-and-forth exchanges.

## P

**.pack File**: PackC format for distributing prompt templates and configurations.

**PackC**: PromptKit tool for packaging and versioning prompt templates.

**Pipeline**: Sequence of middleware components that process LLM requests.

**Prompt**: Input text sent to an LLM, including instructions and context.

**PromptArena**: PromptKit tool for testing and evaluating LLM applications.

**Provider**: LLM service (OpenAI, Anthropic, Google) abstracted behind common interface.

## R

**Rate Limiting**: Restricting the number of API requests within a time period.

**Redis**: In-memory data store used for persistent state management in production.

**Response**: Output generated by an LLM in reply to a prompt.

**Retry**: Automatic re-attempt of failed LLM API calls with exponential backoff.

**Role**: Message attribute indicating sender (system, user, assistant, tool).

**Runtime**: Core PromptKit library for building LLM applications with pipelines.

## S

**SDK**: Higher-level PromptKit library providing simplified conversation management.

**Session**: Identifier grouping related conversation messages together.

**State Management**: Maintaining conversation history across multiple turns.

**State Store**: Component that persists conversation messages (in-memory or Redis).

**Streaming**: Receiving LLM responses incrementally as they're generated.

**System Prompt**: Initial instruction that defines LLM behavior and personality.

## T

**Temperature**: Configuration parameter controlling LLM randomness (0=deterministic, 2=creative).

**Template**: Reusable prompt structure with variable placeholders.

**Token**: Basic unit of text processing (~4 characters or ~0.75 words in English).

**Tool**: External function that LLMs can call to perform actions or retrieve data.

**Tool Call**: LLM request to execute a specific tool with parameters.

**TTL (Time-To-Live)**: Duration before stored data expires and is automatically deleted.

## U

**Usage**: Token consumption data returned by provider (input tokens, output tokens, total).

**User**: Role in message format representing end-user input.

## V

**Validation**: Checking content for safety, quality, and compliance before/after LLM processing.

**Validator**: Component that implements validation logic and can reject messages.

**Variable**: Placeholder in templates replaced with actual values at runtime.

## Z

**Zero-Shot**: Prompting technique without examples, relying only on instructions.

---

## Component-Specific Terms

### Runtime

- **ExecutionContext**: Container for request data passed through pipeline
- **Middleware**: Modular pipeline components (State, Template, Validator, Provider)
- **StateMiddleware**: Manages conversation history loading and saving
- **TemplateMiddleware**: Applies prompt templates with variable substitution
- **ValidatorMiddleware**: Enforces guardrails on input/output

### SDK

- **Conversation**: High-level abstraction for managing chat interactions
- **ConversationConfig**: Configuration for SDK conversations (model, temperature, etc.)

### PromptArena

- **Arena**: Testing environment for LLM evaluation
- **Test Suite**: Collection of test cases in arena.yaml
- **Assertion**: Condition that must be true for test to pass
- **Comparison**: Side-by-side evaluation of multiple providers

### PackC

- **Pack**: Bundle of prompt templates in .pack format
- **Prompt File**: YAML file defining template structure (.prompt extension)
- **Template Registry**: Collection of registered prompt templates

### MCP

- **MCP Server**: External process providing tools via Model Context Protocol
- **MCP Client**: PromptKit component connecting to MCP servers
- **StdioClient**: MCP client using standard input/output communication
- **Tool Discovery**: Process where LLM learns available tools from MCP server

## Related Documentation

- [Core Concepts](concepts/index.md) - Detailed explanations of key concepts
- [Architecture](architecture/index.md) - System design and structure
- [Getting Started](guides/getting-started.md) - Introduction to PromptKit
- [API Reference](runtime/reference/index.md) - Complete API documentation
